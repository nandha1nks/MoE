MoE LLM: https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/

Scaling Vision with Sparse Mixture of Experts: https://arxiv.org/abs/2106.05974
Mixture-of-Expert Conformer for Streaming Multilingual ASR: https://arxiv.org/abs/2305.15663
FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting: https://arxiv.org/pdf/2201.12740.pdf
